{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4e9W4l3BmaSmElPpcJUKZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rxhith/NLP/blob/main/Nlpexp4_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIuGNKm0ac18",
        "outputId": "c269e792-102f-4a1d-f113-deb1b5082f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Define a list of categorical labels\n",
        "labels = ['dog', 'cat', 'bird', 'cat', 'dog', 'bird']\n",
        "\n",
        "# Create a OneHotEncoder object\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Fit the encoder to the labels and transform the labels\n",
        "onehot = encoder.fit_transform(np.array(labels).reshape(-1, 1)).toarray()\n",
        "\n",
        "# Print the OneHot encoded labels\n",
        "print(onehot)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Load a script as a string\n",
        "script = \"This is a sample script. It contains multiple words that we want to count.\"\n",
        "\n",
        "# Tokenize the script into individual words\n",
        "words = word_tokenize(script)\n",
        "\n",
        "# Compute the frequency distribution of the words\n",
        "freq_dist = FreqDist(words)\n",
        "\n",
        "# Print the 10 most common words in the script\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAVAtFZCa9HE",
        "outputId": "6a054734-f8ce-4709-f8a2-56665f085207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 2), ('This', 1), ('is', 1), ('a', 1), ('sample', 1), ('script', 1), ('It', 1), ('contains', 1), ('multiple', 1), ('words', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load a script as a string\n",
        "script = \"This is a sample script. It contains multiple words that we want to filter out, like 'a' and 'the'.\"\n",
        "\n",
        "# Tokenize the script into individual words\n",
        "words = word_tokenize(script)\n",
        "\n",
        "# Define the words to filter out\n",
        "stopwords = ['a', 'an', 'the']\n",
        "\n",
        "# Create a list of words that don't match the filter words\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "\n",
        "# Join the filtered words back into a string\n",
        "filtered_script = ' '.join(filtered_words)\n",
        "\n",
        "# Print out the filtered script\n",
        "print(filtered_script)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoOQCOHqbzMj",
        "outputId": "56ba5326-78b1-4dde-dc78-a5696e56b223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is sample script . It contains multiple words that we want to filter out , like ' ' and 'the ' .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Load a script as a string\n",
        "script = \"This is a complete sentence. This sentence is incomplete\"\n",
        "\n",
        "# Tokenize the script into sentences\n",
        "sentences = sent_tokenize(script)\n",
        "\n",
        "# Check for incomplete sentences\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    if words[-1] != '.':\n",
        "        print(f\"Incomplete sentence: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NdpNRmKb1Eo",
        "outputId": "cfa7cb93-d5d5-40c3-dc3a-cfaaae26b785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incomplete sentence: This sentence is incomplete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "text = input(\"Enter a sentence: \")\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIU7Xa-NcNHI",
        "outputId": "4684a48c-40c5-4de1-ba53-70a840dd0a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: Hi hello how\n",
            "['Hi', 'hello', 'how']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = input(\"Enter a sentence: \")\n",
        "tokens = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nNiu5vbcXhy",
        "outputId": "ced55e6b-f8f1-4a8a-a16f-61d74ea2fb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: I want to buy an apple phone, but I cannot even buy a regular apple.\n",
            "['want', 'buy', 'apple', 'phone', ',', 'even', 'buy', 'regular', 'apple', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = input(\"Enter a sentence: \")\n",
        "tokens = word_tokenize(text)\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4RerfZRciAU",
        "outputId": "7442fc9d-640f-4766-9d19-dba4949746bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: I could not attend your calls as I was busy gaming.\n",
            "['i', 'could', 'not', 'attend', 'your', 'call', 'as', 'i', 'wa', 'busi', 'game', '.']\n"
          ]
        }
      ]
    }
  ]
}